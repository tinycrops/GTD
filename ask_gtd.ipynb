{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTD RAG System\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system over the Getting Things Done (GTD) book chapters.\n",
    "\n",
    "## Features\n",
    "- Document loading and preprocessing from chapters/ directory\n",
    "- Text chunking for optimal retrieval\n",
    "- Vector embeddings using sentence-transformers\n",
    "- Semantic search and retrieval\n",
    "- Integration with OpenAI for generation\n",
    "- Interactive query interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: sentence-transformers in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (1.7.4)\n",
      "Requirement already satisfied: openai in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (1.82.0)\n",
      "Requirement already satisfied: python-dotenv in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: tiktoken in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: filelock in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ath/miniconda3/envs/sakana/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Required packages installed and imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -qU sentence-transformers faiss-cpu openai python-dotenv tiktoken\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "# For text processing\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For vector storage and retrieval\n",
    "import faiss\n",
    "\n",
    "# For OpenAI integration\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# For display\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"✅ Required packages installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully!\n",
      "   Chapters directory: chapters/\n",
      "   Chunk size: 1000\n",
      "   Embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class RAGConfig:\n",
    "    def __init__(self):\n",
    "        self.chapters_dir = \"chapters/\"\n",
    "        self.chunk_size = 1000\n",
    "        self.chunk_overlap = 200\n",
    "        self.embedding_model = \"all-MiniLM-L6-v2\"\n",
    "        self.max_tokens = 8192\n",
    "        self.top_k = 5\n",
    "        \n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Set up OpenAI API key\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not openai.api_key:\n",
    "            print(\"⚠️  Warning: OPENAI_API_KEY not found in environment variables.\")\n",
    "            print(\"   You can still use the retrieval functionality, but generation will be limited.\")\n",
    "\n",
    "config = RAGConfig()\n",
    "print(\"✅ Configuration loaded successfully!\")\n",
    "print(f\"   Chapters directory: {config.chapters_dir}\")\n",
    "print(f\"   Chunk size: {config.chunk_size}\")\n",
    "print(f\"   Embedding model: {config.embedding_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 markdown files to process...\n",
      "✅ Loaded: chapter-01-the-art-of-getting-things-done.md (8819 words)\n",
      "✅ Loaded: chapter-02-getting-control-of-y-our-life-the-five-steps.md (10388 words)\n",
      "✅ Loaded: chapter-03-getting-projects-creatively-under-w-ay-the.md (8751 words)\n",
      "✅ Loaded: chapter-04-getting-started-setting-up-the-t-ime-space.md (7915 words)\n",
      "✅ Loaded: chapter-05-capturing-corralling-y-our-stuf-f.md (4628 words)\n",
      "✅ Loaded: chapter-06-clarifying-getting-in-to-empty.md (6630 words)\n",
      "✅ Loaded: chapter-07-or-ganizing-setting-up-the-right-buckets.md (17748 words)\n",
      "✅ Loaded: chapter-08-reflecting-keeping-it-all-fresh-and.md (4667 words)\n",
      "✅ Loaded: chapter-09-engaging-making-the-best-action-choices.md (8491 words)\n",
      "✅ Loaded: chapter-10-getting-projects-under-control.md (4032 words)\n",
      "✅ Loaded: chapter-11-the-power-of-the-capturing-habit.md (3830 words)\n",
      "✅ Loaded: chapter-12-the-power-of-the-next-action-decision.md (4613 words)\n",
      "✅ Loaded: chapter-13-the-power-of-outcome-focusing.md (3093 words)\n",
      "✅ Loaded: chapter-14-gtd-and-cognitive-science.md (3391 words)\n",
      "✅ Loaded: chapter-15-the-path-of-gtd-mastery.md (4910 words)\n",
      "\n",
      "📚 Successfully loaded 15 documents\n",
      "   Total words: 101,906\n",
      "\n",
      "📊 Document Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chapter-02-getting-control-of-y-our-life-the-f...</td>\n",
       "      <td>10388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chapter-03-getting-projects-creatively-under-w...</td>\n",
       "      <td>8751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chapter-04-getting-started-setting-up-the-t-im...</td>\n",
       "      <td>7915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chapter-05-capturing-corralling-y-our-stuf-f.md</td>\n",
       "      <td>4628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chapter-06-clarifying-getting-in-to-empty.md</td>\n",
       "      <td>6630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chapter-07-or-ganizing-setting-up-the-right-bu...</td>\n",
       "      <td>17748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chapter-08-reflecting-keeping-it-all-fresh-and.md</td>\n",
       "      <td>4667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chapter-09-engaging-making-the-best-action-cho...</td>\n",
       "      <td>8491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chapter-10-getting-projects-under-control.md</td>\n",
       "      <td>4032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chapter-11-the-power-of-the-capturing-habit.md</td>\n",
       "      <td>3830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chapter-12-the-power-of-the-next-action-decisi...</td>\n",
       "      <td>4613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chapter-13-the-power-of-outcome-focusing.md</td>\n",
       "      <td>3093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>chapter-14-gtd-and-cognitive-science.md</td>\n",
       "      <td>3391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chapter-15-the-path-of-gtd-mastery.md</td>\n",
       "      <td>4910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Chapter  Words\n",
       "0        chapter-01-the-art-of-getting-things-done.md   8819\n",
       "1   chapter-02-getting-control-of-y-our-life-the-f...  10388\n",
       "2   chapter-03-getting-projects-creatively-under-w...   8751\n",
       "3   chapter-04-getting-started-setting-up-the-t-im...   7915\n",
       "4     chapter-05-capturing-corralling-y-our-stuf-f.md   4628\n",
       "5        chapter-06-clarifying-getting-in-to-empty.md   6630\n",
       "6   chapter-07-or-ganizing-setting-up-the-right-bu...  17748\n",
       "7   chapter-08-reflecting-keeping-it-all-fresh-and.md   4667\n",
       "8   chapter-09-engaging-making-the-best-action-cho...   8491\n",
       "9        chapter-10-getting-projects-under-control.md   4032\n",
       "10     chapter-11-the-power-of-the-capturing-habit.md   3830\n",
       "11  chapter-12-the-power-of-the-next-action-decisi...   4613\n",
       "12        chapter-13-the-power-of-outcome-focusing.md   3093\n",
       "13            chapter-14-gtd-and-cognitive-science.md   3391\n",
       "14              chapter-15-the-path-of-gtd-mastery.md   4910"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Document Loading\n",
    "class DocumentLoader:\n",
    "    def __init__(self, chapters_dir: str):\n",
    "        self.chapters_dir = chapters_dir\n",
    "        \n",
    "    def load_documents(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Load all markdown documents from the chapters directory.\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        # Get all markdown files\n",
    "        md_files = glob.glob(os.path.join(self.chapters_dir, \"*.md\"))\n",
    "        \n",
    "        print(f\"Found {len(md_files)} markdown files to process...\")\n",
    "        \n",
    "        for file_path in sorted(md_files):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    \n",
    "                    # Extract filename without extension\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    \n",
    "                    # Skip README file\n",
    "                    if filename.lower() == 'readme.md':\n",
    "                        continue\n",
    "                    \n",
    "                    documents.append({\n",
    "                        'filename': filename,\n",
    "                        'filepath': file_path,\n",
    "                        'content': content,\n",
    "                        'word_count': len(content.split())\n",
    "                    })\n",
    "                    \n",
    "                print(f\"✅ Loaded: {filename} ({len(content.split())} words)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {file_path}: {str(e)}\")\n",
    "                \n",
    "        return documents\n",
    "\n",
    "# Load documents\n",
    "loader = DocumentLoader(config.chapters_dir)\n",
    "documents = loader.load_documents()\n",
    "\n",
    "print(f\"\\n📚 Successfully loaded {len(documents)} documents\")\n",
    "print(f\"   Total words: {sum(doc['word_count'] for doc in documents):,}\")\n",
    "\n",
    "# Display document summary\n",
    "doc_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Chapter': doc['filename'], \n",
    "        'Words': doc['word_count']\n",
    "    } \n",
    "    for doc in documents\n",
    "])\n",
    "print(\"\\n📊 Document Summary:\")\n",
    "display(doc_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing documents into chunks...\n",
      "✅ chapter-01-the-art-of-getting-things-done.md: 12 chunks created\n",
      "✅ chapter-02-getting-control-of-y-our-life-the-five-steps.md: 14 chunks created\n",
      "✅ chapter-03-getting-projects-creatively-under-w-ay-the.md: 12 chunks created\n",
      "✅ chapter-04-getting-started-setting-up-the-t-ime-space.md: 11 chunks created\n",
      "✅ chapter-05-capturing-corralling-y-our-stuf-f.md: 7 chunks created\n",
      "✅ chapter-06-clarifying-getting-in-to-empty.md: 9 chunks created\n",
      "✅ chapter-07-or-ganizing-setting-up-the-right-buckets.md: 24 chunks created\n",
      "✅ chapter-08-reflecting-keeping-it-all-fresh-and.md: 6 chunks created\n",
      "✅ chapter-09-engaging-making-the-best-action-choices.md: 11 chunks created\n",
      "✅ chapter-10-getting-projects-under-control.md: 5 chunks created\n",
      "✅ chapter-11-the-power-of-the-capturing-habit.md: 5 chunks created\n",
      "✅ chapter-12-the-power-of-the-next-action-decision.md: 7 chunks created\n",
      "✅ chapter-13-the-power-of-outcome-focusing.md: 4 chunks created\n",
      "✅ chapter-14-gtd-and-cognitive-science.md: 5 chunks created\n",
      "✅ chapter-15-the-path-of-gtd-mastery.md: 7 chunks created\n",
      "\n",
      "📝 Total chunks created: 139\n",
      "   Average tokens per chunk: 930\n",
      "\n",
      "📊 Sample Chunks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Chunk ID</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>0</td>\n",
       "      <td>987</td>\n",
       "      <td>Chapter 1: The Art of Getting Things Done *Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>1</td>\n",
       "      <td>992</td>\n",
       "      <td>And most people are to some degree frustrated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>And if you could keep life in general more in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>(Even in the 1980s many professionals consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>4</td>\n",
       "      <td>980</td>\n",
       "      <td>The Promise: The “Ready State” of the Martial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>5</td>\n",
       "      <td>979</td>\n",
       "      <td>Y ou probably had a sense of being in control,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>6</td>\n",
       "      <td>994</td>\n",
       "      <td>Now , describe, in a single written sentence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>7</td>\n",
       "      <td>975</td>\n",
       "      <td>—Kerry Gleeson you haven’ t decided what the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>8</td>\n",
       "      <td>992</td>\n",
       "      <td>Stuf f is not inherently a bad thing. Things t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chapter-01-the-art-of-getting-things-done.md</td>\n",
       "      <td>9</td>\n",
       "      <td>968</td>\n",
       "      <td>Clarifying things on the front end, when they ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Source  Chunk ID  Tokens  \\\n",
       "0  chapter-01-the-art-of-getting-things-done.md         0     987   \n",
       "1  chapter-01-the-art-of-getting-things-done.md         1     992   \n",
       "2  chapter-01-the-art-of-getting-things-done.md         2     992   \n",
       "3  chapter-01-the-art-of-getting-things-done.md         3     999   \n",
       "4  chapter-01-the-art-of-getting-things-done.md         4     980   \n",
       "5  chapter-01-the-art-of-getting-things-done.md         5     979   \n",
       "6  chapter-01-the-art-of-getting-things-done.md         6     994   \n",
       "7  chapter-01-the-art-of-getting-things-done.md         7     975   \n",
       "8  chapter-01-the-art-of-getting-things-done.md         8     992   \n",
       "9  chapter-01-the-art-of-getting-things-done.md         9     968   \n",
       "\n",
       "                                             Preview  \n",
       "0  Chapter 1: The Art of Getting Things Done *Par...  \n",
       "1  And most people are to some degree frustrated ...  \n",
       "2  And if you could keep life in general more in ...  \n",
       "3  (Even in the 1980s many professionals consider...  \n",
       "4  The Promise: The “Ready State” of the Martial ...  \n",
       "5  Y ou probably had a sense of being in control,...  \n",
       "6  Now , describe, in a single written sentence, ...  \n",
       "7  —Kerry Gleeson you haven’ t decided what the v...  \n",
       "8  Stuf f is not inherently a bad thing. Things t...  \n",
       "9  Clarifying things on the front end, when they ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text Chunking\n",
    "class TextChunker:\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove markdown headers (keep content)\n",
    "        text = re.sub(r'^#+\\s*', '', text, flags=re.MULTILINE)\n",
    "        # Remove excessive newlines\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def chunk_text(self, text: str, source_info: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Split into sentences for better chunk boundaries\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', cleaned_text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = len(self.tokenizer.encode(sentence))\n",
    "            \n",
    "            # If adding this sentence would exceed chunk size, save current chunk\n",
    "            if current_tokens + sentence_tokens > self.chunk_size and current_chunk:\n",
    "                chunks.append({\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'source': source_info['filename'],\n",
    "                    'filepath': source_info['filepath'],\n",
    "                    'chunk_id': len(chunks),\n",
    "                    'tokens': current_tokens\n",
    "                })\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                if self.chunk_overlap > 0:\n",
    "                    # Take last few sentences for overlap\n",
    "                    overlap_sentences = current_chunk.split('. ')[-2:]\n",
    "                    current_chunk = '. '.join(overlap_sentences) + '. ' + sentence\n",
    "                    current_tokens = len(self.tokenizer.encode(current_chunk))\n",
    "                else:\n",
    "                    current_chunk = sentence\n",
    "                    current_tokens = sentence_tokens\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'source': source_info['filename'],\n",
    "                'filepath': source_info['filepath'],\n",
    "                'chunk_id': len(chunks),\n",
    "                'tokens': current_tokens\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_documents(self, documents: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Process all documents and return chunks.\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        print(\"🔄 Processing documents into chunks...\")\n",
    "        \n",
    "        for doc in documents:\n",
    "            chunks = self.chunk_text(doc['content'], doc)\n",
    "            all_chunks.extend(chunks)\n",
    "            print(f\"✅ {doc['filename']}: {len(chunks)} chunks created\")\n",
    "        \n",
    "        print(f\"\\n📝 Total chunks created: {len(all_chunks)}\")\n",
    "        print(f\"   Average tokens per chunk: {np.mean([chunk['tokens'] for chunk in all_chunks]):.0f}\")\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "# Create text chunks\n",
    "chunker = TextChunker(config.chunk_size, config.chunk_overlap)\n",
    "chunks = chunker.process_documents(documents)\n",
    "\n",
    "# Display chunk statistics\n",
    "chunk_stats = pd.DataFrame([\n",
    "    {\n",
    "        'Source': chunk['source'],\n",
    "        'Chunk ID': chunk['chunk_id'],\n",
    "        'Tokens': chunk['tokens'],\n",
    "        'Preview': chunk['text'][:100] + \"...\" if len(chunk['text']) > 100 else chunk['text']\n",
    "    }\n",
    "    for chunk in chunks[:10]  # Show first 10 chunks\n",
    "])\n",
    "\n",
    "print(\"\\n📊 Sample Chunks:\")\n",
    "display(chunk_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading embedding model: all-MiniLM-L6-v2\n",
      "✅ Model loaded! Embedding dimension: 384\n",
      "🔄 Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc01a6c6d2cf45c6987a02a7f13ead80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed batch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4cff95baab410d9729d4b0009f71cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed batch 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f703015bb41d4c5aaf2ec2546ed5130f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed batch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cbd0cd4b484727b2f331b230b18586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed batch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132391675aad4f0d81e0f69d4212d938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed batch 5/5\n",
      "✅ Created 139 embeddings\n",
      "🔄 Building FAISS index...\n",
      "✅ FAISS index built with 139 vectors\n",
      "\n",
      "🎯 Vector store ready!\n",
      "   Index size: 139 vectors\n",
      "   Embedding dimension: 384\n",
      "\n",
      "🔍 Test search: 'What is the two-minute rule?'\n",
      "\n",
      "1. [chapter-06-clarifying-getting-in-to-empty.md] (Score: 0.485)\n",
      "   That’ s a rather dramatic testimonial, but it’ s an indication of just how critical some of these simple processing behaviors can be, especially as the volume and speed of the input increase for you p...\n",
      "\n",
      "2. [chapter-09-engaging-making-the-best-action-choices.md] (Score: 0.253)\n",
      "   Y ou have three pages of scribbled notes from the conversation. There’ s a meeting scheduled with your staf f at eleven, about half an hour from now .. Y ou were out late last night with your spouse’ ...\n",
      "\n",
      "3. [chapter-01-the-art-of-getting-things-done.md] (Score: 0.225)\n",
      "   —Kerry Gleeson you haven’ t decided what the very next physical action step is; and/or you haven’ t put reminders of the outcome and the action required in a system you trust. That’ s why it’ s on you...\n"
     ]
    }
   ],
   "source": [
    "# Embedding and Vector Storage\n",
    "class VectorStore:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        print(f\"🔄 Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        print(f\"✅ Model loaded! Embedding dimension: {self.dimension}\")\n",
    "        \n",
    "    def create_embeddings(self, chunks: List[Dict[str, str]]) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for all chunks.\"\"\"\n",
    "        print(\"🔄 Creating embeddings...\")\n",
    "        \n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        # Create embeddings in batches to manage memory\n",
    "        batch_size = 32\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "            embeddings.append(batch_embeddings)\n",
    "            print(f\"   Processed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
    "        \n",
    "        embeddings = np.vstack(embeddings)\n",
    "        print(f\"✅ Created {embeddings.shape[0]} embeddings\")\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def build_index(self, chunks: List[Dict[str, str]]):\n",
    "        \"\"\"Build FAISS index from chunks.\"\"\"\n",
    "        self.chunks = chunks\n",
    "        embeddings = self.create_embeddings(chunks)\n",
    "        \n",
    "        print(\"🔄 Building FAISS index...\")\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)  # Inner product for cosine similarity\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"✅ FAISS index built with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str, any]]:\n",
    "        \"\"\"Search for similar chunks.\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        # Create query embedding\n",
    "        query_embedding = self.model.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.chunks):  # Valid index\n",
    "                result = self.chunks[idx].copy()\n",
    "                result['similarity_score'] = float(score)\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create vector store and build index\n",
    "vector_store = VectorStore(config.embedding_model)\n",
    "vector_store.build_index(chunks)\n",
    "\n",
    "print(f\"\\n🎯 Vector store ready!\")\n",
    "print(f\"   Index size: {vector_store.index.ntotal} vectors\")\n",
    "print(f\"   Embedding dimension: {vector_store.dimension}\")\n",
    "\n",
    "# Test search\n",
    "test_query = \"What is the two-minute rule?\"\n",
    "test_results = vector_store.search(test_query, top_k=3)\n",
    "\n",
    "print(f\"\\n🔍 Test search: '{test_query}'\")\n",
    "for i, result in enumerate(test_results, 1):\n",
    "    print(f\"\\n{i}. [{result['source']}] (Score: {result['similarity_score']:.3f})\")\n",
    "    print(f\"   {result['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Generation System\n",
    "class RAGSystem:\n",
    "    def __init__(self, vector_store: VectorStore, config: RAGConfig):\n",
    "        self.vector_store = vector_store\n",
    "        self.config = config\n",
    "        self.client = openai.OpenAI() if openai.api_key else None\n",
    "        \n",
    "    def retrieve_context(self, query: str, top_k: int = None) -> str:\n",
    "        \"\"\"Retrieve relevant context for a query.\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config.top_k\n",
    "            \n",
    "        results = self.vector_store.search(query, top_k)\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            context_parts.append(\n",
    "                f\"**Source {i}: {result['source']}** (Relevance: {result['similarity_score']:.3f})\\n\"\n",
    "                f\"{result['text']}\\n\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n---\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate a prompt for the language model.\"\"\"\n",
    "        prompt = f\"\"\"You are an AI assistant specialized in David Allen's \"Getting Things Done\" (GTD) methodology. Use the provided context from the GTD book to answer the user's question accurately and comprehensively.\n",
    "\n",
    "**Context from GTD book:**\n",
    "{context}\n",
    "\n",
    "**User Question:** {query}\n",
    "\n",
    "**Instructions:**\n",
    "- Answer based primarily on the provided context\n",
    "- If the context doesn't fully address the question, indicate what information is missing\n",
    "- Use GTD terminology and concepts accurately\n",
    "- Provide practical, actionable advice when appropriate\n",
    "- Maintain David Allen's tone and approach\n",
    "- If you reference specific GTD principles or techniques, explain them clearly\n",
    "\n",
    "**Answer:**\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_answer(self, query: str, max_tokens: int = None) -> Dict[str, any]:\n",
    "        \"\"\"Generate an answer using RAG.\"\"\"\n",
    "        if max_tokens is None:\n",
    "            max_tokens = self.config.max_tokens\n",
    "            \n",
    "        # Retrieve context\n",
    "        context = self.retrieve_context(query)\n",
    "        \n",
    "        # Generate prompt\n",
    "        prompt = self.generate_prompt(query, context)\n",
    "        \n",
    "        result = {\n",
    "            'query': query,\n",
    "            'context': context,\n",
    "            'prompt': prompt,\n",
    "            'answer': None,\n",
    "            'error': None\n",
    "        }\n",
    "        \n",
    "        # Generate answer if OpenAI is available\n",
    "        if self.client:\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a GTD (Getting Things Done) expert assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                result['answer'] = response.choices[0].message.content\n",
    "                \n",
    "            except Exception as e:\n",
    "                result['error'] = f\"OpenAI API error: {str(e)}\"\n",
    "        else:\n",
    "            result['error'] = \"OpenAI API key not configured. Only retrieval available.\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def display_result(self, result: Dict[str, any], show_context: bool = False):\n",
    "        \"\"\"Display RAG result in a formatted way.\"\"\"\n",
    "        print(f\"🔍 **Query:** {result['query']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if result['answer']:\n",
    "            print(\"📝 **Answer:**\")\n",
    "            display(Markdown(result['answer']))\n",
    "        elif result['error']:\n",
    "            print(f\"❌ **Error:** {result['error']}\")\n",
    "        \n",
    "        if show_context:\n",
    "            print(\"\\n📚 **Retrieved Context:**\")\n",
    "            print(\"-\" * 40)\n",
    "            display(Markdown(result['context']))\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = RAGSystem(vector_store, config)\n",
    "\n",
    "# Test the RAG system\n",
    "test_questions = [\n",
    "    \"What is the two-minute rule in GTD?\",\n",
    "    \"How do I organize my GTD system?\",\n",
    "    \"What are the five steps of GTD workflow?\"\n",
    "]\n",
    "\n",
    "print(\"🤖 Testing RAG System:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n💭 Testing: {question}\")\n",
    "    result = rag_system.generate_answer(question)\n",
    "    \n",
    "    if result['answer']:\n",
    "        print(\"✅ Generated answer successfully\")\n",
    "    else:\n",
    "        print(f\"⚠️  {result['error']}\")\n",
    "        print(\"📚 Retrieved context available for manual review\")\n",
    "\n",
    "print(\"\\n🎯 RAG System is ready for use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Query Interface\n",
    "\n",
    "def ask_gtd(question: str, show_context: bool = False, top_k: int = None):\n",
    "    \"\"\"\n",
    "    Ask a question about GTD methodology.\n",
    "    \n",
    "    Args:\n",
    "        question (str): Your question about GTD\n",
    "        show_context (bool): Whether to display the retrieved context\n",
    "        top_k (int): Number of relevant chunks to retrieve (default: 5)\n",
    "    \"\"\"\n",
    "    if top_k:\n",
    "        rag_system.config.top_k = top_k\n",
    "    \n",
    "    result = rag_system.generate_answer(question)\n",
    "    rag_system.display_result(result, show_context=show_context)\n",
    "    return result\n",
    "\n",
    "# Demo queries\n",
    "print(\"🎯 **GTD RAG System - Ready to Answer Your Questions!**\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n📋 **Usage Examples:**\")\n",
    "print(\"1. `ask_gtd('What is the two-minute rule?')`\")\n",
    "print(\"2. `ask_gtd('How do I do a weekly review?', show_context=True)`\")\n",
    "print(\"3. `ask_gtd('What are contexts in GTD?', top_k=3)`\")\n",
    "\n",
    "print(\"\\n💡 **Sample Questions to Try:**\")\n",
    "sample_questions = [\n",
    "    \"What is the two-minute rule in GTD?\",\n",
    "    \"How do I set up a GTD system?\",\n",
    "    \"What is the difference between projects and next actions?\",\n",
    "    \"How often should I do a weekly review?\",\n",
    "    \"What are contexts and how do I use them?\",\n",
    "    \"How do I capture everything in my head?\",\n",
    "    \"What is the GTD workflow process?\",\n",
    "    \"How do I organize my reference materials?\",\n",
    "    \"What is the purpose of the inbox in GTD?\",\n",
    "    \"How do I handle waiting-for items?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(sample_questions, 1):\n",
    "    print(f\"{i:2d}. {q}\")\n",
    "\n",
    "print(f\"\\n🔧 **System Status:**\")\n",
    "print(f\"   📚 Documents loaded: {len(documents)}\")\n",
    "print(f\"   📝 Text chunks: {len(chunks)}\")\n",
    "print(f\"   🔍 Vector index size: {vector_store.index.ntotal}\")\n",
    "print(f\"   🤖 OpenAI integration: {'✅ Ready' if rag_system.client else '❌ Not configured'}\")\n",
    "\n",
    "print(f\"\\n🚀 **Ready to answer your GTD questions!**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Try the RAG System!\n",
    "\n",
    "# Example 1: Basic question\n",
    "print(\"🎬 **Demo 1: Basic GTD Question**\")\n",
    "print(\"=\" * 40)\n",
    "ask_gtd(\"What is the two-minute rule in GTD?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Show Context Retrieval\n",
    "\n",
    "print(\"\\n🎬 **Demo 2: Context Retrieval**\")\n",
    "print(\"=\" * 40)\n",
    "print(\"This shows how the system retrieves relevant context from the GTD chapters:\")\n",
    "\n",
    "# Example with context display\n",
    "ask_gtd(\"How do I organize my next actions?\", show_context=True, top_k=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 **GTD RAG System Complete!**\")\n",
    "print(\"The system is now ready to answer questions about Getting Things Done methodology.\")\n",
    "print(\"Use the `ask_gtd()` function to query the system interactively.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sakana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
